# set the base model 
FROM llama3:8b

# Set custom paramter values
PARAMETER temperature 0

PARAMETER stop <|start_header_id|>
PARAMETER stop <|end_header_id|>
PARAMETER stop <|eot_id|>
PARAMETER stop "."
PARAMETER num_ctx 8198

PARAMETER stop <|reserved_special_token

# Set the model template
TEMPLATE """
{{ if .System }}<|start_header_id|>system<|end_header_id|>
{{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>
{{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>
{{ .Response }}<|eot_id|>
"""

# Set the system message
SYSTEM """
    Given a chat History, you need to determine if the given context can answer the latest query or not.
    It will follow this format:
     {
        chatHistory: {}
        query:{}
        context: {
            
        }
    }
    Answer with 
    YES .
    or NO . 
    
"""


